{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. What is a parameter?\n",
        "\n",
        "**  A parameter is a value or reference that you pass to a function, method, or procedure in programming or a variable in mathematics or science to influence its behavior or outcome.\n",
        "\n",
        "In Programming:\n",
        "A parameter is a variable listed inside the parentheses in a function or method definition. When you call the function, you provide arguments (actual values) that are passed to these parameters.\n",
        "\n",
        "for exp."
      ],
      "metadata": {
        "id": "ZB9c8CllCmeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greet(name):  # 'name' is a parameter\n",
        "    print(f\"Hello, {name}!\")\n"
      ],
      "metadata": {
        "id": "A0K_KG0JC6Hg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. What is correlation? What does negative correlation mean?"
      ],
      "metadata": {
        "id": "3LLHFnFhDQX6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Correlation is a statistical measure that describes the strength and direction of a relationship between two variables.\n",
        "\n",
        "Key Points:\n",
        "\n",
        "Positive correlation: As one variable increases, the other tends to increase.\n",
        "\n",
        "Example: Height and weight—taller people tend to weigh more.\n",
        "\n",
        "Negative correlation: As one variable increases, the other tends to decrease.\n",
        "\n",
        "Example: Number of hours spent watching TV and grades—more TV might be linked to lower grades.\n",
        "\n",
        "No correlation: No consistent relationship between the variables.\n",
        "\n",
        "Numerical Measure: Correlation Coefficient (r)\n",
        "Ranges from -1 to +1:\n",
        "\n",
        "+1: Perfect positive correlation\n",
        "\n",
        "0: No correlation\n",
        "\n",
        "-1: Perfect negative correlation\n",
        "\n",
        "The most common method for calculating it is Pearson’s correlation coefficient, which assumes a linear relationship between the variables.\n",
        "\n",
        "Important Note:\n",
        "Correlation does not imply causation. Just because two variables are correlated doesn’t mean one causes the other."
      ],
      "metadata": {
        "id": "XVTECeF_EAQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Define Machine Learning. What are the main components in Machine Learning?"
      ],
      "metadata": {
        "id": "NgPPrsYwER70"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Machine Learning is a subfield of Artificial Intelligence (AI) that focuses on developing algorithms and statistical models that enable computers to perform tasks without being explicitly programmed. Instead, systems learn patterns from data and make decisions or predictions based on it.\n",
        "\n",
        "In simple terms:\n",
        "\n",
        "Machine Learning is the science of enabling computers to learn from data and improve their performance over time without being manually programmed.\n",
        "\n",
        "Main Components of Machine Learning\n",
        "* Data\n",
        "\n",
        "The foundational element.\n",
        "\n",
        "Can be structured (e.g., spreadsheets, databases) or unstructured (e.g., text, images, videos).\n",
        "\n",
        "Quality and quantity of data significantly impact model performance.\n",
        "\n",
        "* Features\n",
        "\n",
        "Measurable properties or characteristics of the data.\n",
        "\n",
        "Also known as input variables or predictors.\n",
        "\n",
        "Feature engineering (creating new features or modifying existing ones) is crucial for effective learning.\n",
        "\n",
        "* Model\n",
        "\n",
        "A mathematical or statistical structure that represents the relationship between inputs (features) and outputs (targets).\n",
        "\n",
        "Common models include linear regression, decision trees, neural networks, etc.\n",
        "\n",
        "* Algorithm\n",
        "\n",
        "The method used to train the model.\n",
        "\n",
        "Determines how the model learns from the data.\n",
        "\n",
        "Examples: Gradient Descent, Backpropagation, k-Means, etc.\n",
        "\n",
        "* Training\n",
        "\n",
        "The process of feeding data into a model and adjusting the model parameters to minimize error.\n",
        "\n",
        "This is where the model “learns.”\n",
        "\n",
        "* Evaluation\n",
        "\n",
        "Assessing how well the model performs using metrics like accuracy, precision, recall, F1-score, RMSE, etc.\n",
        "\n",
        "Often involves splitting data into training and testing (or validation) sets.\n",
        "\n",
        "* Prediction / Inference\n",
        "\n",
        "Once trained, the model can make predictions or decisions based on new, unseen data.\n",
        "\n",
        "* Loss Function\n",
        "\n",
        "Measures how far off the model's predictions are from actual values.\n",
        "\n",
        "Guides the training process to improve accuracy.\n",
        "\n",
        "Examples: Mean Squared Error (MSE), Cross-Entropy Loss.\n",
        "\n",
        "* Optimization\n",
        "\n",
        "The process of adjusting model parameters to minimize the loss function.\n",
        "\n",
        "Optimization algorithms (like stochastic gradient descent) play a key role here.\n",
        "\n",
        "Optional but Important Components\n",
        "\n",
        "Hyperparameters: Settings that define the model structure or training process (e.g., learning rate, number of trees).\n",
        "\n",
        "Overfitting/Underfitting: Key concepts in evaluating model generalization.\n",
        "\n",
        "Regularization: Techniques to reduce overfitting by penalizing complex models."
      ],
      "metadata": {
        "id": "ESp7PjwbEr8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 4. How does loss value help in determining whether the model is good or not?"
      ],
      "metadata": {
        "id": "5X0eHRfeDHUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** The loss value is a key metric in evaluating how well a machine learning model is performing. It quantifies the difference between the model’s predictions and the actual target values. Here's how it helps determine whether a model is good or not:\n",
        "\n",
        "✅ What Loss Value Tells You:\n",
        "Indicates Prediction Error:\n",
        "\n",
        "A lower loss means your model's predictions are closer to the actual values (i.e., better performance).\n",
        "\n",
        "A higher loss indicates larger errors in predictions.\n",
        "\n",
        "Guides Model Training:\n",
        "\n",
        "During training, the loss value helps adjust model parameters using optimization algorithms like gradient descent.\n",
        "\n",
        "A steadily decreasing loss over epochs typically means the model is learning well.\n",
        "\n",
        "Enables Model Comparison:\n",
        "\n",
        "You can compare the loss values of different models or architectures on the same dataset to decide which one performs better.\n",
        "\n",
        "⚠️ But Loss Value Alone Isn't Enough:\n",
        "While loss is useful, it has limitations and should be considered alongside other metrics:\n",
        "\n",
        "Type of Problem\tComplementary Metrics\n",
        "\n",
        "Classification\tAccuracy, Precision, Recall, F1 Score\n",
        "Regression\tMean Absolute Error (MAE), R² Score\n",
        "Imbalanced Data\tROC-AUC, Precision-Recall Curve\n",
        "\n",
        "For example:\n",
        "\n",
        "A model might have a low loss but poor accuracy in classification if it's overfitting or if the dataset is imbalanced.\n",
        "\n",
        "In regression, a low loss might not capture how well the model generalizes to unseen data.\n",
        "\n",
        "✅ Best Practices:\n",
        "Monitor both training and validation loss to check for overfitting (e.g., training loss decreasing, validation loss increasing).\n",
        "\n",
        "Use early stopping if the validation loss stops improving to avoid overfitting.\n",
        "\n",
        "Combine loss value with other performance metrics specific to your task for a well-rounded evaluation.\n",
        "\n"
      ],
      "metadata": {
        "id": "GUMy33Y2GWH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "eiFf1oipGnlr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** 1. Continuous Variables\n",
        "\n",
        "Definition:\n",
        "A continuous variable can take any value within a range. These values are typically measured, not counted, and they can be infinitely precise depending on the measuring tool.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Height (e.g., 172.3 cm)\n",
        "\n",
        "Weight (e.g., 65.5 kg)\n",
        "\n",
        "Temperature (e.g., 36.6°C)\n",
        "\n",
        "Time (e.g., 3.25 seconds)\n",
        "\n",
        "Income (e.g., $52,483.75)\n",
        "\n",
        "Key Characteristics:\n",
        "\n",
        "Numeric\n",
        "\n",
        "Can take decimals or fractions\n",
        "\n",
        "Often represented with interval or ratio scales\n",
        "\n",
        "2. Categorical Variables\n",
        "\n",
        "Definition:\n",
        "A categorical variable represents distinct groups or categories. These are typically labels or names and cannot be meaningfully measured on a numeric scale.\n",
        "\n",
        "Types of Categorical Variables:\n",
        "\n",
        "Nominal: Categories without any order\n",
        "e.g., Gender (male, female), Color (red, blue, green)\n",
        "\n",
        "Ordinal: Categories with a meaningful order, but not evenly spaced\n",
        "e.g., Education level (high school, college, graduate)\n",
        "\n",
        "Examples:\n",
        "\n",
        "Blood type (A, B, AB, O)\n",
        "\n",
        "Marital status (single, married, divorced)\n",
        "\n",
        "Product category (electronics, clothing, groceries)\n",
        "\n",
        "Key Characteristics:\n",
        "\n",
        "Non-numeric (or numeric codes with no mathematical meaning)\n",
        "\n",
        "Represent distinct groups or labels\n",
        "\n"
      ],
      "metadata": {
        "id": "LTM1EdesG_1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. How do we handle categorical variables in Machine Learning? What are the common techniques?"
      ],
      "metadata": {
        "id": "l5dtYTTJHeu0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Handling categorical variables is an essential part of preparing data for Machine Learning models, especially those that require numerical input. Here's a breakdown of common techniques used to encode or transform categorical variables:\n",
        "\n",
        " Key Techniques to Handle Categorical Variables\n",
        "\n",
        "1. Label Encoding it does Converts each category into a unique integer.\n",
        "\n",
        "Use when: Categories have an ordinal relationship (e.g., \"low\", \"medium\", \"high\").\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "DS7tZSmGHuzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Size: [Small, Medium, Large] → [0, 1, 2]\n"
      ],
      "metadata": {
        "id": "mXrVC-F6IUV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. One-Hot Encoding\n",
        "What it does: Creates binary columns for each category.\n",
        "\n",
        "Use when: Categories are nominal (no order).\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "qeZCS7MfIgpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Color: [Red, Blue, Green] → [1 0 0], [0 1 0], [0 0 1]\n",
        "\n",
        "# Tools: pandas.get_dummies(), sklearn.preprocessing.OneHotEncoder"
      ],
      "metadata": {
        "id": "Fb4iu2knIrxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Ordinal Encoding: Similar to label encoding, but used deliberately for ordinal variables.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "FOdgxU4uI7Kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Education: [High School, Bachelor's, Master's, PhD] → [1, 2, 3, 4]\n",
        "\n",
        "#Best for: Models that can interpret order like decision trees.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dc_W1c8PJGKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Target Encoding : Replaces categories with the mean of the target variable for each category.\n",
        "\n",
        "Use when: Large number of categories, and supervised learning.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "7g2HAKluJPCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Category A → average target: 0.7, Category B → 0.3\n",
        "\n",
        "#Caveat: Risk of data leakage; use cross-validation to mitigate.\n"
      ],
      "metadata": {
        "id": "ewzEbjV8JdKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Frequency Encoding: Replaces categories with their frequency (or count) in the dataset.\n",
        "\n",
        "Use Large cardinality with no meaningful order."
      ],
      "metadata": {
        "id": "jrqxy7huJpNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Binary Encoding / Hashing\n",
        "\n",
        "Binary Encoding: Encodes categories into binary digits and uses those as features.\n",
        "\n",
        "Hashing Encoding: Hashes the category name to a fixed number of columns (hash trick).\n",
        "\n",
        "Use in High cardinality (e.g., 1000+ unique categories).\n",
        "\n",
        "Tools: CategoryEncoders library"
      ],
      "metadata": {
        "id": "teNog3IkJ1BH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. What do you mean by training and testing a dataset?"
      ],
      "metadata": {
        "id": "0tE7c04MJ8NW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Training a Dataset\n",
        "\n",
        "It means we use a portion of your data called the training set to teach a machine learning model.\n",
        "\n",
        "Goal: The model learns patterns, relationships, or rules in the data.\n",
        "\n",
        "Example: If you're building a model to predict house prices, the training data might include house sizes, locations, and past sale prices. The model uses this data to understand how those features relate to price.\n",
        "\n",
        "2. Testing a Dataset\n",
        "\n",
        "It means After training, we evaluate the model's performance on a different subset of the data called the test set.\n",
        "\n",
        "Goal: To check how well the model generalizes to new, unseen data.\n",
        "\n",
        "Example: we input house details the model hasn't seen before and check how accurately it predicts the price."
      ],
      "metadata": {
        "id": "3b7jUdmhKaeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "ESZH0kQuK6s6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** sklearn.preprocessing is a module in scikit-learn, a popular machine learning library in Python. This module provides various utility functions and classes for scaling, transforming, and normalizing data—steps that are often crucial before feeding data into a machine learning model.\n",
        "\n",
        "* Common Tasks Handled by sklearn.preprocessing\n",
        "\n",
        "Here are the key preprocessing tasks and the tools used:\n",
        "\n",
        "1. Feature Scaling\n",
        "\n",
        "Ensures that features are on a similar scale.\n",
        "\n",
        "StandardScaler: Standardizes features by removing the mean and scaling to unit variance (Z-score normalization).\n",
        "\n",
        "MinMaxScaler: Scales features to a given range (default 0 to 1).\n",
        "\n",
        "MaxAbsScaler: Scales each feature by its maximum absolute value (good for sparse data).\n",
        "\n",
        "RobustScaler: Scales features using statistics that are robust to outliers.\n",
        "\n",
        "2. Normalization\n",
        "\n",
        "Scales input vectors individually to unit norm (L1 or L2).\n",
        "\n",
        "normalize: A function that normalizes samples row-wise.\n",
        "\n",
        "3. Encoding Categorical Features\n",
        "\n",
        "Converts categorical variables into numeric formats.\n",
        "\n",
        "OneHotEncoder: Converts categorical features to one-hot encoded format.\n",
        "\n",
        "OrdinalEncoder: Converts categories into integers.\n",
        "\n",
        "LabelEncoder: Encodes target labels with value between 0 and n_classes-1 (used for target variable).\n",
        "\n",
        "4. Generating Polynomial Features\n",
        "\n",
        "PolynomialFeatures: Expands features into polynomial combinations.\n",
        "\n",
        "5. Custom and Miscellaneous Transformers\n",
        "\n",
        "FunctionTransformer: Wraps a custom function to be used as a transformer.\n",
        "\n",
        "PowerTransformer: Applies power transforms like Box-Cox or Yeo-Johnson.\n",
        "\n",
        "Binarizer: Thresholds numerical features to binary (0/1).\n",
        "\n"
      ],
      "metadata": {
        "id": "n9EIF9ZLLCd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = [[1, 2], [3, 4], [5, 6]]\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n"
      ],
      "metadata": {
        "id": "wAKf0jkfL4Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. What is a Test set?"
      ],
      "metadata": {
        "id": "gGIqQw75Lw98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** A test set is a subset of data used to evaluate the performance of a trained machine learning model. It's not used during the training process, which helps ensure that the evaluation reflects how well the model will perform on new, unseen data.\n",
        "\n",
        "In context:\n",
        "\n",
        "When building a machine learning model, the dataset is often split into three parts:\n",
        "\n",
        "Training set: Used to train the model.\n",
        "\n",
        "Validation set (optional): Used to tune model hyperparameters and prevent overfitting.\n",
        "\n",
        "Test set: Used only after training is complete, to assess the final model's accuracy, precision, recall, etc."
      ],
      "metadata": {
        "id": "2Z1L4MzaMMMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. How do we split data for model fitting (training and testing) in Python?How do you approach a Machine Learning problem?"
      ],
      "metadata": {
        "id": "jkPubA-5MXvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** 1. Split Data for Model Fitting in Python\n",
        "\n",
        "To split your data into training and testing sets, you typically use the train_test_split function from scikit-learn:"
      ],
      "metadata": {
        "id": "uWjzhZ1jMe_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assume X is your features and y is your target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "NQEp7QzoNMZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test_size=0.2: 20% of the data is used for testing, 80% for training.\n",
        "\n",
        "random_state: ensures reproducibility of the split.\n",
        "\n",
        "We can also use stratified splitting to maintain class balance:"
      ],
      "metadata": {
        "id": "-5iJgW8CNLm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "FDRMjv3HNaZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. How to Approach a Machine Learning Problem\n",
        "\n",
        "A structured approach helps in solving ML problems efficiently:\n",
        "\n",
        "Step 1: Understand the Problem\n",
        "\n",
        "Identify if it's a classification, regression, clustering, etc.\n",
        "\n",
        "Understand the business or research objective.\n",
        "\n",
        "Step 2: Acquire and Explore Data\n",
        "\n",
        "Load data using pandas, numpy, etc.\n",
        "\n",
        "Check for missing values, data types, outliers, and class imbalance.\n",
        "\n",
        "Use df.describe(), df.info(), and visualizations (matplotlib, seaborn).\n",
        "\n",
        "Step 3: Preprocess the Data\n",
        "\n",
        "Handle missing values (SimpleImputer, fillna).\n",
        "\n",
        "Encode categorical variables (LabelEncoder, OneHotEncoder).\n",
        "\n",
        "Scale features (StandardScaler, MinMaxScaler).\n",
        "\n",
        "Feature engineering if applicable.\n",
        "\n",
        "Step 4: Split Data\n",
        "\n",
        "Use train_test_split to separate training and test data.\n",
        "\n",
        "Consider using cross-validation (KFold, StratifiedKFold).\n",
        "\n",
        "Step 5: Choose and Train Models\n",
        "\n",
        "Try baseline models like LogisticRegression, RandomForest, XGBoost, etc.\n",
        "\n",
        "Fit models using .fit() on training data.\n",
        "\n",
        "Step 6: Evaluate Model\n",
        "\n",
        "Use metrics:\n",
        "\n",
        "Classification: accuracy_score, precision, recall, f1-score, confusion_matrix\n",
        "\n",
        "Regression: mean_squared_error, r2_score\n",
        "\n",
        "Plot ROC curves, precision-recall curves, etc.\n",
        "\n",
        "Step 7: Tune Hyperparameters\n",
        "\n",
        "Use GridSearchCV or RandomizedSearchCV for model tuning.\n",
        "\n",
        "Step 8: Final Model and Deployment\n",
        "\n",
        "Retrain on full dataset if needed.\n",
        "\n",
        "Save model (joblib, pickle).\n",
        "\n",
        "Deploy using Flask, FastAPI, or cloud platforms."
      ],
      "metadata": {
        "id": "y06rxgajNn23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Why do we have to perform EDA before fitting a model to the data?"
      ],
      "metadata": {
        "id": "KAFzS7eGN1Sw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Performing Exploratory Data Analysis (EDA) before fitting a model is crucial because it helps you understand your data thoroughly and set yourself up for better modeling. Here are the key reasons why EDA is important:\n",
        "\n",
        "Understand Data Structure and Patterns\n",
        "\n",
        "EDA helps you get a feel for your data — its size, types of variables (categorical, numerical), distributions, missing values, outliers, and relationships between variables. This understanding guides your modeling choices.\n",
        "\n",
        "Detect and Handle Missing or Incorrect Data\n",
        "\n",
        "Missing values, inconsistencies, or errors in data can severely impact model performance. EDA helps identify these issues so you can decide how to handle them (imputation, removal, correction).\n",
        "\n",
        "Identify Outliers and Anomalies\n",
        "\n",
        "Outliers can distort your model. EDA helps spot outliers and understand whether they are errors, rare events, or valid extreme values, so you can decide the right approach.\n",
        "\n",
        "Feature Engineering Insights\n",
        "\n",
        "By examining correlations and distributions, you can create or transform features (e.g., log transforms, binning) that might improve your model.\n",
        "\n",
        "Check Assumptions\n",
        "\n",
        "Many models have assumptions (e.g., normality, linearity, independence). EDA helps test these assumptions and decide if data transformations or different models are needed.\n",
        "\n",
        "Improve Model Selection\n",
        "\n",
        "Understanding the relationships and complexity of data helps you select appropriate models (linear, tree-based, etc.) and avoid overfitting or underfitting.\n",
        "\n",
        "Communicate Insights\n",
        "\n",
        "EDA visualizations and summaries help you and stakeholders understand the data story, which is important for making informed decisions."
      ],
      "metadata": {
        "id": "ZKrb0P6iN-w4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. What is correlation?"
      ],
      "metadata": {
        "id": "khDF_fJZOgUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Correlation is a statistical measure that describes the strength and direction of a relationship between two variables. In other words, it tells you how one variable changes when the other variable changes.\n",
        "\n",
        "Key points about correlation:\n",
        "\n",
        "Direction: Correlation can be positive (both variables increase or decrease together) or negative (one variable increases while the other decreases).\n",
        "\n",
        "Strength: The strength of the relationship is measured by a correlation coefficient, usually denoted as r, which ranges from -1 to +1.\n",
        "\n",
        "r = +1: Perfect positive correlation (variables move exactly together).\n",
        "\n",
        "r = -1: Perfect negative correlation (variables move exactly opposite).\n",
        "\n",
        "r = 0: No linear correlation (no linear relationship).\n",
        "\n"
      ],
      "metadata": {
        "id": "MZevWYXBO6Kr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. What does negative correlation mean?"
      ],
      "metadata": {
        "id": "NtfwinzKO9s6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Negative correlation means that two variables tend to move in opposite directions. When one variable increases, the other tends to decrease, and vice versa.\n",
        "\n",
        "For example, if you look at hours spent watching TV and test scores, a negative correlation might mean that as TV watching time goes up, test scores tend to go down.\n",
        "\n",
        "The strength of this relationship is measured by a correlation coefficient ranging from -1 to 1:\n",
        "\n",
        "-1 means a perfect negative correlation (variables move exactly opposite).\n",
        "\n",
        "0 means no correlation (variables don’t have any consistent relationship).\n",
        "\n",
        "1 means a perfect positive correlation (variables move exactly together)."
      ],
      "metadata": {
        "id": "_NtZ1oRSPSRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14. How can you find correlation between variables in Python?"
      ],
      "metadata": {
        "id": "DyP0HLytPYys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** To find the correlation between variables in Python, you typically use the pandas or numpy library."
      ],
      "metadata": {
        "id": "Odr8QwZ2PfLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using pandas.DataFrame.corr()\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'A': [1, 2, 3, 4, 5],\n",
        "    'B': [2, 4, 6, 8, 10],\n",
        "    'C': [5, 3, 4, 2, 1]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "print(correlation_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPiH-xlQP3lW",
        "outputId": "d56215aa-a3de-4000-db25-a795b7c9e572"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     A    B    C\n",
            "A  1.0  1.0 -0.9\n",
            "B  1.0  1.0 -0.9\n",
            "C -0.9 -0.9  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Default method is Pearson correlation.\n",
        "\n",
        "You can also specify method: 'pearson', 'kendall', 'spearman'."
      ],
      "metadata": {
        "id": "eNri66UsQQtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.corr(method='spearman')\n"
      ],
      "metadata": {
        "id": "SeuxZFgTQIrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Using scipy.stats"
      ],
      "metadata": {
        "id": "F8KD5Ur8QaYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "x = [1, 2, 3, 4, 5]\n",
        "y = [2, 4, 6, 8, 10]\n",
        "\n",
        "# Pearson correlation\n",
        "corr, _ = pearsonr(x, y)\n",
        "print(\"Pearson correlation:\", corr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITbfLsLOQbfl",
        "outputId": "4b2a2f1e-2bb0-4c4d-b197-f7701bdd1261"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pearson correlation: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spearman correlation\n",
        "corr, _ = spearmanr(x, y)\n",
        "print(\"Spearman correlation:\", corr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tg0FNLZgQwa6",
        "outputId": "0761321d-7395-4267-9d62-322dec8c2bd6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spearman correlation: 0.9999999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Using numpy.corrcoef()"
      ],
      "metadata": {
        "id": "W9zRZErHQ3Hb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([2, 4, 6, 8, 10])\n",
        "\n",
        "corr_matrix = np.corrcoef(x, y)\n",
        "print(corr_matrix)"
      ],
      "metadata": {
        "id": "QWgXWWuXQ5J9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 15. What is causation? Explain difference between correlation and causation with an example."
      ],
      "metadata": {
        "id": "sUQtFYRTRBEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Causation refers to a relationship between two events or variables where one directly affects the other. In simple terms, causation means that a change in one variable causes a change in another.\n",
        "\n",
        "# Correlation vs. Causation\n",
        "Correlation is a statistical relationship between two variables. It means they move together (either up or down), but it doesn’t necessarily mean that one causes the other.\n",
        "\n",
        "Causation means that one event is the result of the occurrence of the other event – there is a cause-and-effect relationship.\n",
        "\n",
        "Example: Ice Cream Sales & Drowning Incidents\n",
        "\n",
        "Correlation: Studies may show that ice cream sales and drowning incidents increase at the same time. This is a positive correlation.\n",
        "\n",
        "But Causation? No. Buying ice cream doesn't cause drowning. The causal factor here is summer weather – people swim more and buy more ice cream in hot weather.\n",
        "\n",
        "So: Ice cream causes drowning → No causation\n",
        "\n",
        "* Hot weather increases both ice cream sales and swimming → Common cause leading to correlation\n",
        "\n",
        "* Summary\n",
        "\n",
        "Correlation\tTwo variables show a relationship, but one doesn't necessarily cause the other.\n",
        "\n",
        "Causation\tOne variable directly affects another — there's a cause-and-effect.\n",
        "\n",
        "Always remember: Correlation does not imply causation."
      ],
      "metadata": {
        "id": "lVDcwmtFRIPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 16. What is an Optimizer? What are different types of optimizers? Explain each with an example."
      ],
      "metadata": {
        "id": "iWcxDJs0RzFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** An optimizer in machine learning is an algorithm or method used to adjust the parameters (weights and biases) of a model to minimize the loss function during training. The goal is to find the set of parameters that results in the lowest possible error when the model makes predictions.\n",
        "\n",
        "\n",
        "During training, the model makes predictions, compares them to actual outputs using a loss function, and the optimizer updates the weights to reduce the error. This is often done using gradient descent or its variants.\n",
        "\n",
        "# Types of Optimizers\n",
        "Optimizers can be broadly categorized into two types:\n",
        "\n",
        "Gradient Descent-Based Optimizers\n",
        "\n",
        "Heuristic or Evolutionary Optimizers (less common in deep learning)\n",
        "\n",
        "We'll focus on the gradient-based ones, which are more widely used in deep learning.\n",
        "\n",
        "# Common Gradient Descent-Based Optimizers\n",
        "1. Stochastic Gradient Descent (SGD)\n",
        "Idea: Updates weights using the gradient of the loss function with respect to one or a few training examples.\n",
        "\n",
        "Formula:\n",
        "\n",
        "𝜃\n",
        "=\n",
        "𝜃\n",
        "−\n",
        "𝜂\n",
        "⋅\n",
        "∇\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "θ=θ−η⋅∇J(θ)\n",
        "where:\n",
        "\n",
        "𝜃\n",
        "θ: weights\n",
        "\n",
        "𝜂\n",
        "η: learning rate\n",
        "\n",
        "∇\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "∇J(θ): gradient of the loss function\n",
        "\n",
        "Pros: Simple, less memory usage\n",
        "\n",
        "Cons: Can be slow to converge, gets stuck in local minima\n",
        "\n",
        "Example:\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "2. Momentum\n",
        "\n",
        "Idea: Adds a fraction of the previous weight update to the current one, helping to accelerate in the right direction and reduce oscillation.\n",
        "\n",
        "Formula:\n",
        "\n",
        "𝑣\n",
        "𝑡\n",
        "=\n",
        "𝛾\n",
        "𝑣\n",
        "𝑡\n",
        "−\n",
        "1\n",
        "+\n",
        "𝜂\n",
        "∇\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "v\n",
        "t\n",
        "​\n",
        " =γv\n",
        "t−1\n",
        "​\n",
        " +η∇J(θ)\n",
        "𝜃\n",
        "=\n",
        "𝜃\n",
        "−\n",
        "𝑣\n",
        "𝑡\n",
        "θ=θ−v\n",
        "t\n",
        "​\n",
        "\n",
        "Pros: Faster convergence than plain SGD\n",
        "\n",
        "Example:\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "3. AdaGrad (Adaptive Gradient Algorithm)\n",
        "\n",
        "Idea: Adapts the learning rate for each parameter individually, reducing it over time based on past gradients.\n",
        "\n",
        "Pros: Works well for sparse data\n",
        "\n",
        "Cons: Learning rate becomes too small over time\n",
        "\n",
        "Example:\n",
        "\n",
        "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01)\n",
        "\n",
        "4. RMSProp (Root Mean Square Propagation)\n",
        "\n",
        "Idea: Modifies AdaGrad to avoid aggressive decrease in learning rate. It uses a moving average of squared gradients.\n",
        "\n",
        "Pros: Good for recurrent neural networks and non-stationary problems\n",
        "\n",
        "Example:\n",
        "\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
        "\n",
        "5. Adam (Adaptive Moment Estimation)\n",
        "\n",
        "Idea: Combines Momentum and RMSProp; uses moving averages of both gradients and their squares.\n",
        "\n",
        "Formula:\n",
        "\n",
        "𝑚\n",
        "𝑡\n",
        "=\n",
        "𝛽\n",
        "1\n",
        "𝑚\n",
        "𝑡\n",
        "−\n",
        "1\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝛽\n",
        "1\n",
        ")\n",
        "∇\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "m\n",
        "t\n",
        "​\n",
        " =β\n",
        "1\n",
        "​\n",
        " m\n",
        "t−1\n",
        "​\n",
        " +(1−β\n",
        "1\n",
        "​\n",
        " )∇J(θ)\n",
        "𝑣\n",
        "𝑡\n",
        "=\n",
        "𝛽\n",
        "2\n",
        "𝑣\n",
        "𝑡\n",
        "−\n",
        "1\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝛽\n",
        "2\n",
        ")\n",
        "(\n",
        "∇\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        ")\n",
        "2\n",
        "v\n",
        "t\n",
        "​\n",
        " =β\n",
        "2\n",
        "​\n",
        " v\n",
        "t−1\n",
        "​\n",
        " +(1−β\n",
        "2\n",
        "​\n",
        " )(∇J(θ))\n",
        "2\n",
        "\n",
        "𝜃\n",
        "=\n",
        "𝜃\n",
        "−\n",
        "𝜂\n",
        "𝑚\n",
        "𝑡\n",
        "𝑣\n",
        "𝑡\n",
        "+\n",
        "𝜖\n",
        "θ=θ−η\n",
        "v\n",
        "t\n",
        "​\n",
        "\n",
        "​\n",
        " +ϵ\n",
        "m\n",
        "t\n",
        "​\n",
        "\n",
        "​\n",
        "\n",
        "Pros: Generally works well out of the box; most commonly used optimizer today\n",
        "\n",
        "Example:\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "6. AdamW (Adam with Weight Decay)\n",
        "\n",
        "Idea: A variant of Adam that decouples weight decay from gradient updates.\n",
        "\n",
        "Pros: Better regularization, improved generalization\n",
        "\n",
        "Example:\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-2)"
      ],
      "metadata": {
        "id": "MTQ4zdiLTaGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 17. What is sklearn.linear_model ?"
      ],
      "metadata": {
        "id": "UvSNV6kDUfvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** sklearn.linear_model is a module in the scikit-learn library (commonly imported as sklearn) that contains a collection of linear models for regression and classification tasks. These models assume a linear relationship between input features and the target variable.\n",
        "\n",
        "Common Classes in sklearn.linear_model:\n",
        "For Regression:\n",
        "LinearRegression\n",
        "Ordinary least squares linear regression.\n",
        "\n",
        "Ridge\n",
        "Linear regression with L2 regularization (penalizes large coefficients).\n",
        "\n",
        "Lasso\n",
        "Linear regression with L1 regularization (can lead to sparse models).\n",
        "\n",
        "ElasticNet\n",
        "Combines L1 and L2 regularization.\n",
        "\n",
        "SGDRegressor\n",
        "Linear model optimized using stochastic gradient descent.\n",
        "\n",
        "For Classification:\n",
        "LogisticRegression\n",
        "Logistic regression for binary or multiclass classification.\n",
        "\n",
        "RidgeClassifier\n",
        "Ridge regression adapted for classification.\n",
        "\n",
        "SGDClassifier\n",
        "Linear classifiers (e.g., logistic regression, SVM) optimized using stochastic gradient descent.\n",
        "\n",
        "Perceptron\n",
        "A basic linear classifier, similar to SGDClassifier with a specific loss function.\n",
        "\n",
        "Example"
      ],
      "metadata": {
        "id": "KTDKJdiHVBHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data\n",
        "X = [[1], [2], [3], [4]]\n",
        "y = [2, 4, 6, 8]\n",
        "\n",
        "# Create and fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict([[5]])\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSBAREAsVPFV",
        "outputId": "1c646720-82aa-458f-c7b8-c4836f7428a3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When to Use:\n",
        "\n",
        "Use models from sklearn.linear_model when:\n",
        "\n",
        "You suspect or want to test for a linear relationship.\n",
        "\n",
        "You need interpretable models (especially with Lasso or ElasticNet).\n",
        "\n",
        "You want to use regularization to prevent overfitting."
      ],
      "metadata": {
        "id": "RstOv-opVLWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** The model.fit() function in machine learning libraries like Keras (from TensorFlow) is used to train a model. It updates the model's weights based on input data and target outputs by minimizing a loss function using optimization algorithms like gradient descent.\n",
        "\n",
        "It performs the following:\n",
        "\n",
        "Feeds input data (features and labels) to the model.\n",
        "\n",
        "Computes predictions using the current model weights.\n",
        "\n",
        "Calculates the loss (difference between predictions and actual labels).\n",
        "\n",
        "Backpropagates the error and updates the model weights using the chosen optimizer.\n",
        "\n",
        "Repeats this for a number of epochs (passes over the full dataset).\n",
        "\n",
        " Required Arguments\n",
        "\n",
        "In Keras / TensorFlow, the typical syntax is:"
      ],
      "metadata": {
        "id": "5h-22sgHVcR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x, y, ...)\n"
      ],
      "metadata": {
        "id": "muoEW1AcV0UN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 19. What does model.predict() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "rB8GFsxjWCcY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** The method model.predict() is used in machine learning frameworks like TensorFlow/Keras, scikit-learn, and others to make predictions on new input data after a model has been trained.\n",
        "\n",
        "It returns the predicted output(s) for a given input. The format and type of the output depend on the type of model and the framework you're using:\n",
        "\n",
        "In classification tasks: It typically returns probabilities or class scores.\n",
        "\n",
        "In regression tasks: It returns numerical predictions.\n",
        "\n",
        "In neural networks: It may return logits, class scores, or probability distributions depending on the output layer.\n",
        "\n",
        " Required Arguments:\n",
        "\n",
        "The arguments you need to provide depend on the library. Here's a breakdown by framework:\n"
      ],
      "metadata": {
        "id": "eqntSFwXWIKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(x, batch_size=None, verbose=0, steps=None)\n"
      ],
      "metadata": {
        "id": "QK5uLRITXslu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Required:\n",
        "\n",
        "x: Input data. Can be:\n",
        "\n",
        "NumPy array\n",
        "\n",
        "Tensor\n",
        "\n",
        "List of arrays (if multiple inputs)\n",
        "\n",
        "A generator or tf.data dataset"
      ],
      "metadata": {
        "id": "o4JME5ibXt6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scikit-learn\n",
        "\n",
        "predictions = model.predict(X)"
      ],
      "metadata": {
        "id": "Nb6jzSjsX8hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Required:\n",
        "\n",
        "X: Input features (NumPy array, DataFrame, or similar).\n",
        "\n",
        "Note: In classification tasks, you might also use model.predict_proba(X) for probability estimates instead of class labels."
      ],
      "metadata": {
        "id": "X9YUGwUGYJUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# keras\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(10, input_shape=(5,), activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "\n",
        "x_input = np.random.rand(4, 5)  # 4 samples, 5 features each\n",
        "y_pred = model.predict(x_input)\n"
      ],
      "metadata": {
        "id": "622vV2AoYPpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 20. What are continuous and categorical variables?\n"
      ],
      "metadata": {
        "id": "RGHYzJLpYl8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** . Continuous Variables:\n",
        "\n",
        "Definition: Variables that can take an infinite number of values within a given range.\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "Typically measurable.\n",
        "\n",
        "Can have decimal values.\n",
        "\n",
        "Have a natural order.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Height (e.g., 170.5 cm)\n",
        "\n",
        "Weight (e.g., 65.2 kg)\n",
        "\n",
        "Temperature (e.g., 36.6°C)\n",
        "\n",
        "Time (e.g., 2.35 seconds)\n",
        "\n",
        "Age (when measured precisely)\n",
        "\n",
        "2. Categorical Variables:\n",
        "\n",
        "Definition: Variables that represent distinct groups or categories.\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "Typically non-numeric (but can be coded numerically).\n",
        "\n",
        "Values are discrete and usually countable.\n",
        "\n",
        "May or may not have a meaningful order.\n",
        "\n",
        "Types:\n",
        "\n",
        "Nominal (no inherent order):\n",
        "\n",
        "Examples: Gender (male, female), Eye color (blue, green, brown), Marital status (single, married)\n",
        "\n",
        "Ordinal (has a clear order, but differences between categories are not measurable):\n",
        "\n",
        "Examples: Education level (high school, bachelor's, master's), Customer satisfaction (poor, fair, good, excellent)\n",
        "\n"
      ],
      "metadata": {
        "id": "Qf9XCvPXYsfx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 21. What is feature scaling? How does it help in Machine Learning?"
      ],
      "metadata": {
        "id": "ArJ5ZAhtZBH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Feature scaling is a technique in machine learning preprocessing where the values of numerical features are normalized or standardized so that they share a common scale. This is especially important when features have different units or ranges, such as height in centimeters and weight in kilograms.\n",
        "\n",
        "Feature Scaling Is Important in ML:\n",
        "\n",
        "Improves convergence in gradient-based algorithms:\n",
        "\n",
        "Algorithms like gradient descent (used in logistic regression, neural networks, etc.) converge faster when features are scaled similarly.\n",
        "\n",
        "Without scaling, features with larger ranges dominate the cost function, leading to inefficient training.\n",
        "\n",
        "Essential for distance-based algorithms:\n",
        "\n",
        "Algorithms like K-Nearest Neighbors (KNN), K-Means, and Support Vector Machines (SVM) use distance calculations (like Euclidean distance), which are sensitive to the scale of features.\n",
        "\n",
        "Unscaled features can bias the model toward those with larger numerical values.\n",
        "\n",
        "Improves model interpretability:\n",
        "\n",
        "In regularized models (e.g., Lasso, Ridge), scaling ensures that penalty terms affect each feature equally, leading to more meaningful feature importance.\n",
        "\n",
        "Use Feature Scaling:\n",
        "\n",
        "SVM, KNN, PCA, neural networks, logistic regression.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mtkpa2JgZU3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 22. How do we perform scaling in Python?"
      ],
      "metadata": {
        "id": "TUDiNgEyZtFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** In Python, scaling typically refers to transforming data to fit within a specific range or standardizing it to have certain statistical properties (e.g. zero mean and unit variance). This is a common step in data preprocessing for machine learning.\n",
        "\n",
        "Here are the most common ways to perform scaling using scikit-learn, a widely used machine learning library:\n",
        "\n",
        "1. Standardization (Z-score Normalization)\n",
        "This scales the data so that it has a mean of 0 and a standard deviation of 1."
      ],
      "metadata": {
        "id": "eUzFhRXiZ0PO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[1.0, 2.0], [3.0, 6.0], [5.0, 10.0]])\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRyZ8RdlaFzt",
        "outputId": "a0e7fc25-0975-425a-e590-0eb20fe3b1f9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Min-Max Scaling\n",
        "\n",
        "Scales the data to a specified range, usually between 0 and 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "gdG5P4faaM1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPdexkxtaHUX",
        "outputId": "c3338a06-3d1c-4f78-f48e-b953307fbc7a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.  0. ]\n",
            " [0.5 0.5]\n",
            " [1.  1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. MaxAbs Scaling\n",
        "\n",
        "Scales each feature by its maximum absolute value. Useful for data that is already centered at zero.\n",
        "\n"
      ],
      "metadata": {
        "id": "QSqzduOtaXTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHlY914saSMK",
        "outputId": "7bea52f6-e5d0-4f2e-b07b-9fbb631b6174"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.2 0.2]\n",
            " [0.6 0.6]\n",
            " [1.  1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Robust Scaling\n",
        "\n",
        "Uses the median and the interquartile range (IQR), making it robust to outliers."
      ],
      "metadata": {
        "id": "pkNILR2pai9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "scaler = RobustScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYOu7Jk-adaM",
        "outputId": "8ce867e6-c5d3-40ba-8af6-b09155f427b5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1. -1.]\n",
            " [ 0.  0.]\n",
            " [ 1.  1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Manual Scaling with NumPy (not recommended for ML pipelines)\n",
        "You can also scale manually using NumPy:"
      ],
      "metadata": {
        "id": "fHLZ-qT7aoS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.array([[1.0, 2.0], [3.0, 6.0], [5.0, 10.0]])\n",
        "mean = data.mean(axis=0)\n",
        "std = data.std(axis=0)\n",
        "\n",
        "scaled_data = (data - mean) / std\n",
        "print(scaled_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jz6G-ieEam37",
        "outputId": "3957dccb-7796-4115-8c72-beda47bf00e5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 23. What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "gQSxpiY4bEwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** sklearn.preprocessing is a module in scikit-learn (a popular machine learning library in Python) that provides utility functions and classes for scaling, normalizing, and transforming features before feeding them into machine learning models.\n",
        "\n",
        "Proper preprocessing helps improve model performance, training speed, and stability.\n",
        "\n",
        "Key Functionality in sklearn.preprocessing:\n",
        "1. Scaling Features\n",
        "Ensures features are on the same scale.\n",
        "\n",
        "StandardScaler: Standardizes features by removing the mean and scaling to unit variance (Z-score normalization).\n",
        "\n",
        "MinMaxScaler: Scales features to a specified range, typically [0, 1].\n",
        "\n",
        "RobustScaler: Uses median and interquartile range—robust to outliers.\n",
        "\n",
        "MaxAbsScaler: Scales features by their maximum absolute value—good for sparse data.\n",
        "\n",
        "2. Normalization\n",
        "Rescales individual samples to unit norm.\n",
        "\n",
        "normalize(): L2 or L1 normalization applied row-wise.\n",
        "\n",
        "Normalizer: Same as above, but as a transformer.\n",
        "\n",
        "3. Encoding Categorical Variables\n",
        "Converts non-numeric data to numeric.\n",
        "\n",
        "OneHotEncoder: Converts categorical features into a one-hot numeric array.\n",
        "\n",
        "OrdinalEncoder: Converts categorical features to integer codes.\n",
        "\n",
        "LabelEncoder: Encodes target labels (not for features).\n",
        "\n",
        "4. Binarization\n",
        "Binarizer: Converts numerical values to 0 or 1 based on a threshold.\n",
        "\n",
        "5. Polynomial Features\n",
        "PolynomialFeatures: Generates polynomial and interaction features.\n",
        "\n",
        "6. Custom Transformers\n",
        "FunctionTransformer: Allows applying a custom function to transform data.\n",
        "\n",
        "Example"
      ],
      "metadata": {
        "id": "b_qWLukTbKDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "ELs6bvzGavhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 24. How do we split data for model fitting (training and testing) in Python?"
      ],
      "metadata": {
        "id": "niFBfMnzbppj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** In Python, a common and straightforward way to split data into training and testing sets is by using the train_test_split function from scikit-learn (sklearn). This lets you randomly split your dataset while controlling the proportion for training and testing.\n",
        "\n",
        "exp."
      ],
      "metadata": {
        "id": "0EYy_iKnbzel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Suppose X is your feature matrix and y is your target variable\n",
        "X = ...  # your features\n",
        "y = ...  # your labels\n",
        "\n",
        "# Split data: 80% train, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Now you can use X_train, y_train for model training\n",
        "# and X_test, y_test for evaluation\n"
      ],
      "metadata": {
        "id": "DoeBXg5ScCcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 25. Explain data encoding?"
      ],
      "metadata": {
        "id": "ZFL41LsDcKr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Data encoding is the process of converting data from one form to another according to a specific set of rules. This transformation makes data easier to store, transmit, or process by computers and communication systems.\n",
        "\n",
        "Data Encoding Important:\n",
        "\n",
        "Storage Efficiency: Encoded data can take up less space.\n",
        "\n",
        "Transmission: Encoded data is often easier or safer to send over networks.\n",
        "\n",
        "Compatibility: Different systems or software may require data in a certain format.\n",
        "\n",
        "Security: Some encoding methods add a layer of security or obfuscation.\n",
        "\n",
        "Error Detection: Certain encodings help detect or correct errors in data transmission.\n",
        "\n",
        "Common Types of Data Encoding\n",
        "\n",
        "Text Encoding:\n",
        "\n",
        "Converts characters to binary data (bits) so computers can understand text.\n",
        "\n",
        "Examples: ASCII, UTF-8, UTF-16\n",
        "\n",
        "For example, the letter “A” in ASCII is encoded as 01000001.\n",
        "\n",
        "Binary Encoding:\n",
        "Represents data using binary digits (0s and 1s).\n",
        "\n",
        "Base Encoding:\n",
        "Encodes binary data into readable characters for safe transmission over text-based protocols.\n",
        "\n",
        "Examples: Base64, Base32, Base16 (Hexadecimal)\n",
        "\n",
        "Audio/Video Encoding:\n",
        "Converts multimedia data into compressed formats for storage or streaming.\n",
        "\n",
        "Examples: MP3, AAC (audio), H.264, MPEG-4 (video)\n",
        "\n",
        "URL Encoding:\n",
        "Encodes special characters in URLs into a format that can be safely transmitted over the internet.\n",
        "\n",
        "Simple Example\n",
        "Original text: \"Hi\"\n",
        "\n",
        "ASCII encoding:\n",
        "\n",
        "'H' = 72 (decimal) = 01001000 (binary)\n",
        "\n",
        "'i' = 105 (decimal) = 01101001 (binary)\n",
        "\n",
        "So \"Hi\" encoded in ASCII binary is: 01001000 01101001\n",
        "\n"
      ],
      "metadata": {
        "id": "VkHPw4FdcRCu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7QgYr0iccrxe"
      }
    }
  ]
}